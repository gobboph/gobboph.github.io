<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Roberto Gobbetti</title><link href="/" rel="alternate"></link><link href="/feeds/all.atom.xml" rel="self"></link><id>/</id><updated>2017-02-15T14:00:00-05:00</updated><entry><title>What Happened There?</title><link href="/blog/wht/" rel="alternate"></link><updated>2017-02-15T14:00:00-05:00</updated><author><name>Roberto Gobbetti</name></author><id>tag:,2017-02-15:blog/wht/</id><summary type="html">&lt;p&gt;Time series are omnipresent. If you were around in 2008, you probably have seen the graph of a financial index collapsing from one day to another. Alternatively you might check the temperature prediction for each hour of the day before leaving in the morning. A time series is nothing more than a set of values with a time stamp and represents how a certain quantity changes over time.&lt;/p&gt;
&lt;p&gt;Time series often have very recognizable features (e.g. a big drop in the stock prices). The number of daily views of a Wikipedia webpage (its traffic) usually displays big spikes if people show particular interest in its main topic, but it can be hard to figure out why people were interested in a particular person or entity, especially time after the triggering event happened.&lt;/p&gt;
&lt;p&gt;Here is where &lt;a href="http://whathappenedthere.xyz/"&gt;WhatHappenedThere?&lt;/a&gt; enters the scene. Given a topic, the app downloads the time series of its associated Wikipedia page (if it exists) and automatically detects the spikes. It then queries a database storing all the New York Times articles since the earliest time in the Wikipedia time series (July 2015) and returns a list of most relevant articles according to a natural language processing (NLP) algorithm. I will use this post to flesh out some of the technical details behind the project.&lt;/p&gt;
&lt;p&gt;A bit of context: I worked on this project during my fellowship at Insight Data Science and discussed the idea and developments with Matthew Lipson at About.com, who provided great feedback throughout. The development of the app took a couple of weeks from idea to completion and if it works well now, there are definitely directions where it could be improved. As will be clear by the end of this post, WhatHappenedThere? provides a proof of concept for the power of connecting a time series with a source of context to automatically provide insights. For even more details, all of this project is shared on &lt;a href="https://github.com/gobboph/WhatHappenedThere"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As one inputs an entry on WhatHappenedThere?, the app looks for the associated Wikipedia page. If this exists, it then retrieve the daily view counts of the page, i.e. the time series (check out the &lt;a href="https://github.com/gobboph/WhatHappenedThere/blob/master/application/flasknews/wiki.py"&gt;wiki&lt;/a&gt; module on GitHub). The app consists of 3 separate parts joined together:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h4&gt;&lt;a href="#spikes"&gt;Spike detection in Time Series&lt;/a&gt;&lt;/h4&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h4&gt;&lt;a href="#search"&gt;Database query&lt;/a&gt;&lt;/h4&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h4&gt;&lt;a href="#nlp"&gt;NLP on retrieved documents&lt;/a&gt;&lt;/h4&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a name="spikes"&gt;Spike detection&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The spike detection algorithm is coded up in the &lt;a href="https://github.com/gobboph/WhatHappenedThere/blob/master/application/flasknews/anomalies.py"&gt;anomalies&lt;/a&gt; module you can see on GitHub. The algorithm keeps a window period (let's say 30 days) prior to the date I am interested in checking for a spike. It computes the mean and standard deviation and classifies the value as a spike (anomaly) if it falls outside the range of $avg \pm tolerance \times std$. Both the window period and the tolerance are parameters. Notice that thanks to the "-", the algorithm would in principle detect big drops as well, but this is not relevant in this particular case and I can not think of an event that would trigger a drop if not after a spike.&lt;/p&gt;
&lt;p&gt;In the precise case of detecting spikes in webpage traffic, one must bear in mind that the daily distribution of visits has a &lt;a href="https://en.wikipedia.org/wiki/Heavy-tailed_distribution"&gt;heavy tail&lt;/a&gt;, hence the very large spikes we want to detect.  From this point of view, one might think that keeping track of the mean and standard deviation might not be a good approach since the assumption behind this approach is that the distribution of daily views is Gaussian. Still, for all practical purposes, this approach works well as long as the tolerance is kept very high. The app declares a spike when a value is more than 6 standard deviations away from the null hypothesis. For reference, 5 is enough to declare that the Higgs boson does indeed exist.&lt;/p&gt;
&lt;p&gt;Maybe it could be better to treat the daily distribution as a Poissonian. This would not be a big modification: we just need to change the standard deviation to the mean. The net result is similar and again, the app does a posteriori find the peaks that are relevant. I have not thoroughly delved into autoregressive models as they would take longer and defeat the purpose of having a quick and responsive online app, but they are definitely a direction one should explore if interested in developing this further.&lt;/p&gt;
&lt;p&gt;I also added an absolute threshold as a third parameter of the spike detection algorithm. This is useful for my particular application: if a Wikipedia page has 30 views on average and 200 one day, this might as well be a spike, but probably not one that made the news and should be disregarded.&lt;/p&gt;
&lt;h3&gt;&lt;a name="search"&gt;Database search&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The second source of data is a record of all the articles published by the New York Times since July 2015 (the earliest date in the Wikipedia time series). I downloaded the articles using the &lt;a href="https://developer.nytimes.com/"&gt;NYT  Archive API&lt;/a&gt; and stored them with PostgresSQL. The data provided by the API is not complete, meaning the whole body of the article is not provided. Still, there is quite a bit of text in the headline, abstract and lead paragraph, and of course there are the publication dates.&lt;/p&gt;
&lt;p&gt;The search is nothing more than a SQL query that looks for articles within a date bracket (the consecutive dates classified as a spike) and regarding a topic. It is coded up in the &lt;a href="https://github.com/gobboph/WhatHappenedThere/blob/master/application/flasknews/nyt.py"&gt;nyt&lt;/a&gt; module. Although the coding part is in itself trivial, there is an interesting precision vs. recall problem here, &lt;em&gt;i.e.&lt;/em&gt; how to maximize the number of articles that are relevant, while minimizing the number of irrelevant ones that get picked up anyway.&lt;/p&gt;
&lt;p&gt;There is no perfect solution to this problem. Let me explain with an example: I had initially decided to search for each single word in the Wikipedia page name. The reason behind this was that the data for each article is limited and that the NLP part of the project would take care of giving more relevance to the topic I want over others. The drawback of this is clear: if you look for "David Bowie", the algorithm would pick any article referring to anyone called David.&lt;/p&gt;
&lt;p&gt;Alternatively, I tried maximizing precision and searching for the full string. The app would pick up only specific articles, but at the expense of others. An article referring to "President Obama" would not appear, since the app would look for "Barack Obama" only as that is the name tag of the Wikipedia page. In some other case, all possible content might be lost. Looking for "Earthquakes in Italy" would return the Wikipedia page "List of earthquakes in Italy" and no article would have this precise string in it. The previous method would instead return the articles relative to last year's events.&lt;/p&gt;
&lt;p&gt;I eventually settled for precision when I noticed friends were getting frustrated by getting wrong results more than getting no result at all. As of now, the algorithm queries for the full Wikipedia name or the full string inserted by the user. In this way, searching for "Obama" gives more results than searching for "Barack Obama", but you avoid learning about minor politicians when looking for "Michael Jordan".&lt;/p&gt;
&lt;h3&gt;&lt;a name="nlp"&gt;NLP&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The last piece of the picture is extracting meaningful information from the articles. Not having the whole body of the article might look like a problem, but in a sense it helps making things easier: you can think of it as if the New York Times did some part of the job already by weeding out the non essential information. The clear counterpart is that you do not get anything about people relevant enough to be in the news, but not as the main charater of the story.&lt;/p&gt;
&lt;p&gt;An example: "Sergio Mattarella" is the president of Italy and has a big spike in december 2016, the weekend of an important referendum, but still gets no article. The articles that cite "Matteo Renzi" (Italy's prime minister, a more significant role) might cite Mattarella in the body, but he did not make the headline or lead paragraph.&lt;/p&gt;
&lt;p&gt;However one wants to see it, the relatively small amount of data per article, together with the time constraint of computing things on the fly, calls for a fast approach. The app takes all the articles relative to a spike and computes the &lt;a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf"&gt;tf-idf score&lt;/a&gt; of each word ranking them from the most relevant to the least. This helps extracting concepts.&lt;/p&gt;
&lt;p&gt;This helps us find the most relevant articles. In order to do that, we need to define a measure to compute &lt;em&gt;distance&lt;/em&gt; between articles and then find which one has the lowest sum of distances, &lt;em&gt;i.e.&lt;/em&gt; is most similar to most other articles.&lt;/p&gt;
&lt;p&gt;We can treat each article as a vector in the space of all the words of the extracted corpus: it will have a 0 in the direction of a word that does not appear, 1 if the word appears once and so on. I used cosine similarity as a measure. This consists in taking the normalized dot product of two vectors A and B, which is the cosine of the angle between them: the highest this value (closest to 1) the more similar the vectors are. The app computes the cosine similarity between each pair of articles and sums the values for each article: the article that has the highest score is the one whose topics are most similar to most other articles in the batch.&lt;/p&gt;
&lt;p&gt;Finally, the website prints an ordered list of the most relevant 10 articles per spike together with a nice word cloud. You can click on each headline and read the article.&lt;/p&gt;
&lt;h3&gt;Conclusions&lt;/h3&gt;
&lt;p&gt;Perhaps the main drawback of the app is the limitation of the context data, &lt;em&gt;i.e.&lt;/em&gt; the New York Times corpus. The New York Times is a biased source of context: it gives more relevance to American over international news and maybe to some topics over others, like politics. It is not the best publication to figure out what a certain actor or pop star has been up to for example (although "Beyonce" works pretty well). One can easily overcome this limitation with a more diversified dataset.&lt;/p&gt;
&lt;p&gt;At high level, WhatHappenedThere? is a tool to automatically extract information by maintaing a connection between two datasets, a time series and a source of context. This allows to gain precise insights and fast. Of course, a similar tool could help analyze time series beyond Wikipedia traffic and the code is broken down to minimize dependencies and allow for easy adaptation into new projects.&lt;/p&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="data"></category></entry><entry><title>Don't shoot the messenger!</title><link href="/blog/election_2016/" rel="alternate"></link><updated>2016-11-12T18:30:00-05:00</updated><author><name>Roberto Gobbetti</name></author><id>tag:,2016-11-12:blog/election_2016/</id><summary type="html">&lt;p&gt;Despite living in the States on and off for seven years and witnessing the 2012 presidential election, this year I got caught into the campaign madness. I felt the stakes were higher than usual: I had moved back to New York City with the prospect of building a family here but, most importantly, the circus around the election and its rules are truly entertaining for an outsider.&lt;/p&gt;
&lt;p&gt;I am a data geek, and like many of my kind, I became addicted to following how the election predictive models developed throughout the past few months. That is why I am so appalled by the anger so many people seem to direct towards the modelers now that the election is over.&lt;/p&gt;
&lt;p&gt;Articles have been published, even &lt;a href="http://www.nytimes.com/2016/11/10/technology/the-data-said-clinton-would-win-why-you-shouldnt-have-believed-it.html"&gt;in the New York Times&lt;/a&gt;, blasting the models and their makers for not correctly predicting the outcome of the election. These voices seem to confuse a risk model for a deterministic one. The former is what we saw on sites like FiveThirtyEight or even the Times itself, while the second would always give the same outcome given the same initial conditions.&lt;/p&gt;
&lt;p&gt;Let me make this clear with an example. Let’s suppose I play a game of Russian roulette with one bullet, then my probability of survival is about 83%. This is based on the assumption that there are 6 equally probable scenarios and only one kills me. From a frequentist perspective, this means that if 100 people were to play the same game of Russian roulette, around 17 of them would die. If I play it only once and I kill myself, it would be foolish to say that the probability computation was wrong: I was just one of those 17 out of 100.&lt;/p&gt;
&lt;p&gt;This is to say two things: first that it is hard to judge the quality of a model on a single experiment (and we have just one election in 2016). Second, that if someone gives an 80% chance to an event and you take the event for granted, you are making a mistake, not the modeler.&lt;/p&gt;
&lt;p&gt;One sees now that the wild criticism certain models have received might be misplaced. Nate Silver’s FiveThirtyEight model gave something around 70% chance to a Clinton win, with a sizable probability (~12%) of her winning the popular vote despite losing the electoral college. These probabilities are far from being inconceivable.&lt;/p&gt;
&lt;p&gt;If one wants to judge the goodness of FiveThirtyEight’s model, then perhaps they should look at how it did state by state: it predicted most of the swing states within a reasonable error (which was admittedly large this year). Moreover, it predicted Clinton would win the popular vote by ~3%, which turned out to be pretty close to reality.&lt;/p&gt;
&lt;p&gt;I did not look as deeply into the details of the Times’ model, but in the days before the election, it gave Clinton a winning probability of 85%, so we are back to the Russian roulette. Other models gave her a 99% chance: they seem a bit off in retrospect, but one should check how they did state by state and how unlikely they concluded the actual outcome to be.&lt;/p&gt;
&lt;p&gt;Once anyone cooks up any risk model that spits out a probability for a given event, the work is not done yet. The interpretation of the outcome depends on how risky the situation is, and this could be somehow subjective. For example, I would advise against playing Russian roulette with one bullet, but I don’t mind leaving the house without the umbrella if there is a 17% chance of rain. Given the stakes in this election, I personally was freaking out when FiveThirtyEight gave Clinton less than a 70% chance of winning a few days before the election.&lt;/p&gt;
&lt;p&gt;To conclude, it is frustrating to see serious newspapers seemingly struggling to understand the statistical nature of a risk assessment. If all the polls were perfectly conducted (which they were not) and all the modelers had all the right assumptions (which seems unlikely), and we were told that Clinton had 90% chance of winning, this still would imply that there is a scenario in 10 that makes her lose. It is up to us then to decide if we are happy living with that or not.&lt;/p&gt;
&lt;p&gt;PS: for a more in depth analysis on some of these aspects, checkout the latest article on &lt;a href="http://fivethirtyeight.com/features/why-fivethirtyeight-gave-trump-a-better-chance-than-almost-anyone-else/"&gt;FiveThirtyEight&lt;/a&gt;.&lt;/p&gt;</summary><category term="data"></category><category term="social"></category></entry><entry><title>Taxi Ride Value</title><link href="/blog/taxi_rides/" rel="alternate"></link><updated>2016-05-20T22:30:00-04:00</updated><author><name>Roberto Gobbetti</name></author><id>tag:,2016-05-20:blog/taxi_rides/</id><summary type="html">&lt;p&gt;Like many others this past year, I stumbled on the impressive set of &lt;a href="http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml"&gt;TLC Trip Data&lt;/a&gt; and took my turn at checking where the NYC cabs like to hang out. I planned to find the best spots for cabs to pick people up at different times of the day in orer to maximize their hourly income. I will explain here how and show you some plots, you can find code and more images in my &lt;a href="https://github.com/gobboph/nycabs"&gt;github repo&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;These datasets are big. They encompass all the taxi rides from Jan 2009 to Dec 2015. For each month there are O(10^7) rides for which one gets pickup coordinates and datetime, dropoff coordinates and datetime, trip distance, passenger count, total amount paid split in categories, payment method and probably something else I am forgetting now. The files' size oscillates between 1.8-2.0 Gb per month.&lt;/p&gt;
&lt;p&gt;First of all, I downloaded Jan 2015 (literally the first file you see on the website) and here is a cool map of New York City drawn by scatter plotting pickup (in red) and dropoff (green) locations for the whole month.&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img src="../../images/green.png" alt="NYC by cabs" style="width: 800px;"/&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;If you played with this dataset (or with any dataset at all) you know that it needs some cleaning. There appear to be a staggering number of cabs picking people up at the North Pole or traveling at over 1000 miles/h on average. Some even arrive before leaving.&lt;/p&gt;
&lt;p&gt;These are easily taken care of, but what surprised me was that lots of people do not seem to tip: have I been an idiot all this time? The set of customers is divided clearly in tippers and non-tippers, which correspond almost 1 to 1 to card payers and cash payers. I am guessing that inserting the tip in the records when paid in cash is an extra mile that cab drivers are not willing to go for the sake of data collecting.&lt;/p&gt;
&lt;p&gt;I populated the tip column with a linear model based on the card payers and the data was pretty much good to go for further analysis.&lt;/p&gt;
&lt;p&gt;Now, how to assign value to a cab ride? Easiest answer: (total fare) / (ride duration). The problem with this first approximation is that it does not take into account the time that cabs spend finding new customers, i.e. not making money. Still I could not track every single cab to compute the non-paid time with this dataset.&lt;/p&gt;
&lt;p&gt;Luckily some &lt;a href="http://www.andresmh.com/nyctaxitrips/"&gt;old data&lt;/a&gt; has the info needed to track each single taxi. I dowloaded the first set that you find there, i.e. Jan 2013, and that is the base for all that follows.&lt;/p&gt;
&lt;p&gt;After combining and cleaneing this dataset as well, I computed the wait between two clients, here as average per hour of the day. Notice the gigantic variance even after I took care of cabs that are out of service.&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img src="../../images/taxi_2013/wait_jan2013.png" alt="Cabs wait" style="width: 800px;"/&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;Going on, I defined the trip value as:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;trip value = (total fare) / (time waited + trip duration)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;and plotted it against hour of the day and day of the month (weekends and holidays and holidays in red).&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img src="../../images/taxi_2013/valueperhour_jan2013.png" alt="value per hour" style="width: 800px;"/&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img src="../../images/taxi_2013/valueperday_jan2013.png" alt="value per day" style="width: 800px;"/&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;The few cabs out at 5am are doing well (maybe because no one else is around) and then there is a peak after work. From the day of the month barplot we can easily infer that drunk folks like to take cabs after new year's eve celebrations. Also, the drivers seem to make increasingly more $/h as the week progresses.&lt;/p&gt;
&lt;p&gt;Finally, here is a panel with maps (you might want to enlarge it, but a blownup example is below). For each hour of the day the data is pixeled in location and the maps are colored according to the average trip value in each pixel. Pixels with too few cab pickups are discarded: there might be some very valuable trip, but still it is not advisable to drive a particular area if there are just a couple of pickups a month.&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img src="../../images/taxi_2013/panel_jan2013.png" alt="value panel" style="width: 800px;"/&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;As seen above, the middle of the day is not great, but downtown is consistently the best spot. One can also locate JFK and La Guardia airports: they are not necessarily the best spot in town, but they are a sure catch (as expected): the waiting involved probably takes down the trip value even if the total fare is high. Here is a blownup map for 9pm.&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img src="../../images/taxi_2013/example_jan2013_21.png" alt="example map" style="width: 800px;"/&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;Lastly, I checked what parameters matter the most in determining the value of each trip (duration and total fare apart, of course). I trained a random forest and here is the result.&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img src="../../images/taxi_2013/features_jan2013.png" alt="features" style="width: 800px;"/&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;It turns out that cabs make more or less the same money whether it is holiday or not and what matters most is the hour of the day they are out working.&lt;/p&gt;
&lt;p&gt;Location is missing but I am planning to classify the rides by neighborhood. The panel above clearly shows that different neighborhoods will yield different trip values. The most straighforward endgame for this study would be an app that advises cab drivers on where to drive in order to maximize the dollars per hour, given features and location.&lt;/p&gt;
&lt;p&gt;Nevertheless, the obvious next step is to include more than one month worth of data. One month is pretty much all my 8Gb RAM macbook pro can take, but thankfully cloud computing is cheap and easily accessible.&lt;/p&gt;
&lt;p&gt;People have been using these data to implement &lt;a href="http://chriswhong.com/open-data/foil_nyc_taxi/"&gt;really cool visualizations&lt;/a&gt; and answer pressing questions like &lt;a href="http://toddwschneider.com/posts/analyzing-1-1-billion-nyc-taxi-and-uber-trips-with-a-vengeance/"&gt;is Die Hard a realistic movie?&lt;/a&gt;. Yet, I believe there is still a lot of useful projects that could be developed thanks to these datasets. Maximizing cab drivers income is a great possibility, but the real challenge would be to understand what changes our city could implement to make traffic more efficient and all our lives better.&lt;/p&gt;</summary><category term="data"></category><category term="social"></category></entry><entry><title>Election Words</title><link href="/blog/election_words/" rel="alternate"></link><updated>2014-08-04T12:50:00-04:00</updated><author><name>Roberto Gobbetti</name></author><id>tag:,2014-08-04:blog/election_words/</id><summary type="html">&lt;p&gt;Ever wondered what are the best topic to talk about if you want to get elected in congress? And how do they change from state to state? I did and I tried to answer with &lt;a href="http://gobboph.github.io/election_words/"&gt;this&lt;/a&gt; little project of mine.&lt;/p&gt;
&lt;p&gt;I checked who are the folks tha get re-elected a lot in congress and what they talk about. To do that I used again some resources from the &lt;a href="https://sunlightfoundation.com/"&gt;Sunlight Foundation&lt;/a&gt; together with some wikipedia entry (I had to pin down all the people that have passed through congress in the past 20 years somehow).&lt;/p&gt;
&lt;p&gt;This analysis is totally preliminary and can be improved a lot, but it already shows something. First of all it is clear that one has to talk about their own regional problems (cities and boroughs are popular choices). Hovering around the map some topic of federal interest show up: obamacare, fracking, clean energy, iraq. On the other hand it is also clear that certain topics are regional: gambling in the south, HIV in big cities, fishing in the very North East.&lt;/p&gt;
&lt;p&gt;Most importantly though, can anybody explain to me the deal between South Carolina and Bulgaria?&lt;/p&gt;</summary><category term="programming"></category><category term="social"></category></entry><entry><title>Congress Words in Sunlight Foundation Blog</title><link href="/blog/congress_words_3/" rel="alternate"></link><updated>2014-06-21T12:45:00-04:00</updated><author><name>Roberto Gobbetti</name></author><id>tag:,2014-06-21:blog/congress_words_3/</id><summary type="html">&lt;p&gt;Congress Words only works thanks to the &lt;a href="sunlightfoundation.com"&gt;Sunlinght Foundation&lt;/a&gt;, which provides the API's through their wonderful project &lt;a href="capitolwords.org"&gt;capitolwords.org&lt;/a&gt;. A while ago someone at the Sunlight Foundation saw my project and decided it was interesting enough to mention it in their official blog, so &lt;a href="http://sunlightfoundation.com/blog/2014/05/29/use-sunlights-apis-for-your-own-project/"&gt;here&lt;/a&gt; is the entry where they talk about me and Congress Words.&lt;/p&gt;
&lt;p&gt;It is extremely flattering that my little project caught their attention and made it to their blog! I think the foundation is doing an amazing job and I hope I will be able to contribute more in the future.&lt;/p&gt;</summary><category term="programming"></category><category term="social"></category></entry><entry><title>Congress Words Update</title><link href="/blog/conrgess_words_update/" rel="alternate"></link><updated>2014-04-26T20:00:00-04:00</updated><author><name>Roberto Gobbetti</name></author><id>tag:,2014-04-26:blog/conrgess_words_update/</id><summary type="html">&lt;p&gt;A couple of days ago I decided to learn some JavaScript. I have been toying for a while with the idea of extending my little congress_words project to a website, so what better occasion?&lt;/p&gt;
&lt;p&gt;I rewrote the project so it runs entirely on JavaScript, using only jQuery and &lt;a href="http://gobboph.github.io/congress_words/"&gt;here&lt;/a&gt; is the result.&lt;/p&gt;
&lt;p&gt;The script is not too complicated, but it works well and, most importantly, I learned a lot while doing it; now I feel way more confident in my beaing able to learn something quickly. You can find the code at the github page of the &lt;a href="https://github.com/gobboph/congress_words/tree/gh-pages"&gt;project&lt;/a&gt; (in the master branch you can still see the python script).&lt;/p&gt;
&lt;p&gt;Play with &lt;a href="http://gobboph.github.io/congress_words/"&gt;it&lt;/a&gt; and let me know what you think.&lt;/p&gt;</summary><category term="programming"></category><category term="social"></category></entry><entry><title>Unwinding Inflation gets attention</title><link href="/blog/unwinding_inflation/" rel="alternate"></link><updated>2014-04-14T10:35:00-04:00</updated><author><name>Roberto Gobbetti</name></author><id>tag:,2014-04-14:blog/unwinding_inflation/</id><summary type="html">&lt;p&gt;The data release of &lt;a href="http://bicepkeck.org/"&gt;BICEP2&lt;/a&gt; a couple of weeks ago caused much excitement in the physics community and justly so. First of all we were able to detect gravitational waves for the first time, albeit not directly. These gravitational waves were produced in a very early stage of our universe, as predicted by the inflationary paradigm, which in turn gets an important observational confirmation. Although we need confirmation from other experiments, the amount of gravitational waves that has been detected is much more than many people expected, thus ruling out the vast majority of models of inflation based on a microscopic theory of the very high energy (see String Theory).&lt;/p&gt;
&lt;p&gt;The good news is: Unwinding Inflation still stands! The model I developed together with my collaborators is one of the very few that might be able to explain inflation using String Theory. Lots of work still needs to be done on Unwinding to show that it can be embedded naturally into String Theory, but the road seem promising. This is very exciting for us and we are very much looking forward to future developements.&lt;/p&gt;</summary><category term="science"></category></entry><entry><title>Congress Words</title><link href="/blog/conrgess_words/" rel="alternate"></link><updated>2014-04-14T10:30:00-04:00</updated><author><name>Roberto Gobbetti</name></author><id>tag:,2014-04-14:blog/conrgess_words/</id><summary type="html">&lt;p&gt;I spent a night last week having fun while working out this new project on github: &lt;a href="https://github.com/gobboph/congress_words"&gt;congress_words&lt;/a&gt;. Using the API's of &lt;a href="capitolwords.org"&gt;capitolwords.org&lt;/a&gt;, the scripts outputs a map of the US colored accordingly to how many times a given word or sentence of your choice has been pronunced by a representative of each state, of course normalized by number of state representatives.&lt;/p&gt;
&lt;p&gt;I find this to be a fantastic tool for procrastination and I plan to develop it into a website soon (it is important to be able to procrastinate no matter where we are). So come back to it soon!&lt;/p&gt;</summary><category term="programming"></category><category term="social"></category></entry><entry><title>First blog post</title><link href="/blog/first-post/" rel="alternate"></link><updated>2014-03-27T18:30:00-04:00</updated><author><name>Roberto Gobbetti</name></author><id>tag:,2014-03-27:blog/first-post/</id><summary type="html">&lt;p&gt;My personal website goes online! I have been toying with the idea of sitting down and building one for way too long, but finally here it is. This website is built with &lt;a href="http://docs.getpelican.com/en/3.3.0/"&gt;pelican&lt;/a&gt; and is online through my &lt;a href="https://github.com/gobboph"&gt;github page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I will use the blog to write about science, technology, politics, or really whatever I get interested into. Feel free to contact me.&lt;/p&gt;</summary><category term="miscellanea"></category></entry></feed>