<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Roberto Gobbetti</title><link href="/" rel="alternate"></link><link href="/feeds/data.atom.xml" rel="self"></link><id>/</id><updated>2017-02-15T14:00:00-05:00</updated><entry><title>What Happened There?</title><link href="/blog/wht/" rel="alternate"></link><updated>2017-02-15T14:00:00-05:00</updated><author><name>Roberto Gobbetti</name></author><id>tag:,2017-02-15:blog/wht/</id><summary type="html">&lt;p&gt;Time series are omnipresent. If you were around in 2008, you probably have seen the graph of a financial index collapsing from one day to another. Alternatively you might check the temperature prediction for each hour of the day before leaving in the morning. A time series is nothing more than a set of values with a time stamp and represents how a certain quantity changes over time.&lt;/p&gt;
&lt;p&gt;Time series often have very recognizable features (e.g. a big drop in the stock prices). The number of daily views of a Wikipedia webpage (its traffic) usually displays big spikes if people show particular interest in its main topic, but it can be hard to figure out why people were interested in a particular person or entity, especially time after the triggering event happened.&lt;/p&gt;
&lt;p&gt;Here is where &lt;a href="http://whathappenedthere.xyz/"&gt;WhatHappenedThere?&lt;/a&gt; enters the scene. Given a topic, the app downloads the time series of its associated Wikipedia page (if it exists) and automatically detects the spikes. It then queries a database storing all the New York Times articles since the earliest time in the Wikipedia time series (July 2015) and returns a list of most relevant articles according to a natural language processing (NLP) algorithm. I will use this post to flesh out some of the technical details behind the project.&lt;/p&gt;
&lt;p&gt;A bit of context: I worked on this project during my fellowship at Insight Data Science and discussed the idea and developments with Matthew Lipson at About.com, who provided great feedback throughout. The development of the app took a couple of weeks from idea to completion and if it works well now, there are definitely directions where it could be improved. As will be clear by the end of this post, WhatHappenedThere? provides a proof of concept for the power of connecting a time series with a source of context to automatically provide insights. For even more details, all of this project is shared on &lt;a href="https://github.com/gobboph/WhatHappenedThere"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As one inputs an entry on WhatHappenedThere?, the app looks for the associated Wikipedia page. If this exists, it then retrieve the daily view counts of the page, i.e. the time series (check out the &lt;a href="https://github.com/gobboph/WhatHappenedThere/blob/master/application/flasknews/wiki.py"&gt;wiki&lt;/a&gt; module on GitHub). The app consists of 3 separate parts joined together:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h4&gt;&lt;a href="#spikes"&gt;Spike detection in Time Series&lt;/a&gt;&lt;/h4&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h4&gt;&lt;a href="#search"&gt;Database query&lt;/a&gt;&lt;/h4&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;h4&gt;&lt;a href="#nlp"&gt;NLP on retrieved documents&lt;/a&gt;&lt;/h4&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a name="spikes"&gt;Spike detection&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The spike detection algorithm is coded up in the &lt;a href="https://github.com/gobboph/WhatHappenedThere/blob/master/application/flasknews/anomalies.py"&gt;anomalies&lt;/a&gt; module you can see on GitHub. The algorithm keeps a window period (let's say 30 days) prior to the date I am interested in checking for a spike. It computes the mean and standard deviation and classifies the value as a spike (anomaly) if it falls outside the range of $avg \pm tolerance \times std$. Both the window period and the tolerance are parameters. Notice that thanks to the "-", the algorithm would in principle detect big drops as well, but this is not relevant in this particular case and I can not think of an event that would trigger a drop if not after a spike.&lt;/p&gt;
&lt;p&gt;In the precise case of detecting spikes in webpage traffic, one must bear in mind that the daily distribution of visits has a &lt;a href="https://en.wikipedia.org/wiki/Heavy-tailed_distribution"&gt;heavy tail&lt;/a&gt;, hence the very large spikes we want to detect.  From this point of view, one might think that keeping track of the mean and standard deviation might not be a good approach since the assumption behind this approach is that the distribution of daily views is Gaussian. Still, for all practical purposes, this approach works well as long as the tolerance is kept very high. The app declares a spike when a value is more than 6 standard deviations away from the null hypothesis. For reference, 5 is enough to declare that the Higgs boson does indeed exist.&lt;/p&gt;
&lt;p&gt;Maybe it could be better to treat the daily distribution as a Poissonian. This would not be a big modification: we just need to change the standard deviation to the mean. The net result is similar and again, the app does a posteriori find the peaks that are relevant. I have not thoroughly delved into autoregressive models as they would take longer and defeat the purpose of having a quick and responsive online app, but they are definitely a direction one should explore if interested in developing this further.&lt;/p&gt;
&lt;p&gt;I also added an absolute threshold as a third parameter of the spike detection algorithm. This is useful for my particular application: if a Wikipedia page has 30 views on average and 200 one day, this might as well be a spike, but probably not one that made the news and should be disregarded.&lt;/p&gt;
&lt;h3&gt;&lt;a name="search"&gt;Database search&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The second source of data is a record of all the articles published by the New York Times since July 2015 (the earliest date in the Wikipedia time series). I downloaded the articles using the &lt;a href="https://developer.nytimes.com/"&gt;NYT  Archive API&lt;/a&gt; and stored them with PostgresSQL. The data provided by the API is not complete, meaning the whole body of the article is not provided. Still, there is quite a bit of text in the headline, abstract and lead paragraph, and of course there are the publication dates.&lt;/p&gt;
&lt;p&gt;The search is nothing more than a SQL query that looks for articles within a date bracket (the consecutive dates classified as a spike) and regarding a topic. It is coded up in the &lt;a href="https://github.com/gobboph/WhatHappenedThere/blob/master/application/flasknews/nyt.py"&gt;nyt&lt;/a&gt; module. Although the coding part is in itself trivial, there is an interesting precision vs. recall problem here, &lt;em&gt;i.e.&lt;/em&gt; how to maximize the number of articles that are relevant, while minimizing the number of irrelevant ones that get picked up anyway.&lt;/p&gt;
&lt;p&gt;There is no perfect solution to this problem. Let me explain with an example: I had initially decided to search for each single word in the Wikipedia page name. The reason behind this was that the data for each article is limited and that the NLP part of the project would take care of giving more relevance to the topic I want over others. The drawback of this is clear: if you look for "David Bowie", the algorithm would pick any article referring to anyone called David.&lt;/p&gt;
&lt;p&gt;Alternatively, I tried maximizing precision and searching for the full string. The app would pick up only specific articles, but at the expense of others. An article referring to "President Obama" would not appear, since the app would look for "Barack Obama" only as that is the name tag of the Wikipedia page. In some other case, all possible content might be lost. Looking for "Earthquakes in Italy" would return the Wikipedia page "List of earthquakes in Italy" and no article would have this precise string in it. The previous method would instead return the articles relative to last year's events.&lt;/p&gt;
&lt;p&gt;I eventually settled for precision when I noticed friends were getting frustrated by getting wrong results more than getting no result at all. As of now, the algorithm queries for the full Wikipedia name or the full string inserted by the user. In this way, searching for "Obama" gives more results than searching for "Barack Obama", but you avoid learning about minor politicians when looking for "Michael Jordan".&lt;/p&gt;
&lt;h3&gt;&lt;a name="nlp"&gt;NLP&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The last piece of the picture is extracting meaningful information from the articles. Not having the whole body of the article might look like a problem, but in a sense it helps making things easier: you can think of it as if the New York Times did some part of the job already by weeding out the non essential information. The clear counterpart is that you do not get anything about people relevant enough to be in the news, but not as the main charater of the story.&lt;/p&gt;
&lt;p&gt;An example: "Sergio Mattarella" is the president of Italy and has a big spike in december 2016, the weekend of an important referendum, but still gets no article. The articles that cite "Matteo Renzi" (Italy's prime minister, a more significant role) might cite Mattarella in the body, but he did not make the headline or lead paragraph.&lt;/p&gt;
&lt;p&gt;However one wants to see it, the relatively small amount of data per article, together with the time constraint of computing things on the fly, calls for a fast approach. The app takes all the articles relative to a spike and computes the &lt;a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf"&gt;tf-idf score&lt;/a&gt; of each word ranking them from the most relevant to the least. This helps extracting concepts.&lt;/p&gt;
&lt;p&gt;This helps us find the most relevant articles. In order to do that, we need to define a measure to compute &lt;em&gt;distance&lt;/em&gt; between articles and then find which one has the lowest sum of distances, &lt;em&gt;i.e.&lt;/em&gt; is most similar to most other articles.&lt;/p&gt;
&lt;p&gt;We can treat each article as a vector in the space of all the words of the extracted corpus: it will have a 0 in the direction of a word that does not appear, 1 if the word appears once and so on. I used cosine similarity as a measure. This consists in taking the normalized dot product of two vectors A and B, which is the cosine of the angle between them: the highest this value (closest to 1) the more similar the vectors are. The app computes the cosine similarity between each pair of articles and sums the values for each article: the article that has the highest score is the one whose topics are most similar to most other articles in the batch.&lt;/p&gt;
&lt;p&gt;Finally, the website prints an ordered list of the most relevant 10 articles per spike together with a nice word cloud. You can click on each headline and read the article.&lt;/p&gt;
&lt;h3&gt;Conclusions&lt;/h3&gt;
&lt;p&gt;Perhaps the main drawback of the app is the limitation of the context data, &lt;em&gt;i.e.&lt;/em&gt; the New York Times corpus. The New York Times is a biased source of context: it gives more relevance to American over international news and maybe to some topics over others, like politics. It is not the best publication to figure out what a certain actor or pop star has been up to for example (although "Beyonce" works pretty well). One can easily overcome this limitation with a more diversified dataset.&lt;/p&gt;
&lt;p&gt;At high level, WhatHappenedThere? is a tool to automatically extract information by maintaing a connection between two datasets, a time series and a source of context. This allows to gain precise insights and fast. Of course, a similar tool could help analyze time series beyond Wikipedia traffic and the code is broken down to minimize dependencies and allow for easy adaptation into new projects.&lt;/p&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="data"></category></entry><entry><title>Taxi Ride Value</title><link href="/blog/taxi_rides/" rel="alternate"></link><updated>2016-05-20T22:30:00-04:00</updated><author><name>Roberto Gobbetti</name></author><id>tag:,2016-05-20:blog/taxi_rides/</id><summary type="html">&lt;p&gt;Like many others this past year, I stumbled on the impressive set of &lt;a href="http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml"&gt;TLC Trip Data&lt;/a&gt; and took my turn at checking where the NYC cabs like to hang out. I planned to find the best spots for cabs to pick people up at different times of the day in orer to maximize their hourly income. I will explain here how and show you some plots, you can find code and more images in my &lt;a href="https://github.com/gobboph/nycabs"&gt;github repo&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;These datasets are big. They encompass all the taxi rides from Jan 2009 to Dec 2015. For each month there are O(10^7) rides for which one gets pickup coordinates and datetime, dropoff coordinates and datetime, trip distance, passenger count, total amount paid split in categories, payment method and probably something else I am forgetting now. The files' size oscillates between 1.8-2.0 Gb per month.&lt;/p&gt;
&lt;p&gt;First of all, I downloaded Jan 2015 (literally the first file you see on the website) and here is a cool map of New York City drawn by scatter plotting pickup (in red) and dropoff (green) locations for the whole month.&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img src="../../images/green.png" alt="NYC by cabs" style="width: 800px;"/&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;If you played with this dataset (or with any dataset at all) you know that it needs some cleaning. There appear to be a staggering number of cabs picking people up at the North Pole or traveling at over 1000 miles/h on average. Some even arrive before leaving.&lt;/p&gt;
&lt;p&gt;These are easily taken care of, but what surprised me was that lots of people do not seem to tip: have I been an idiot all this time? The set of customers is divided clearly in tippers and non-tippers, which correspond almost 1 to 1 to card payers and cash payers. I am guessing that inserting the tip in the records when paid in cash is an extra mile that cab drivers are not willing to go for the sake of data collecting.&lt;/p&gt;
&lt;p&gt;I populated the tip column with a linear model based on the card payers and the data was pretty much good to go for further analysis.&lt;/p&gt;
&lt;p&gt;Now, how to assign value to a cab ride? Easiest answer: (total fare) / (ride duration). The problem with this first approximation is that it does not take into account the time that cabs spend finding new customers, i.e. not making money. Still I could not track every single cab to compute the non-paid time with this dataset.&lt;/p&gt;
&lt;p&gt;Luckily some &lt;a href="http://www.andresmh.com/nyctaxitrips/"&gt;old data&lt;/a&gt; has the info needed to track each single taxi. I dowloaded the first set that you find there, i.e. Jan 2013, and that is the base for all that follows.&lt;/p&gt;
&lt;p&gt;After combining and cleaneing this dataset as well, I computed the wait between two clients, here as average per hour of the day. Notice the gigantic variance even after I took care of cabs that are out of service.&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img src="../../images/taxi_2013/wait_jan2013.png" alt="Cabs wait" style="width: 800px;"/&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;Going on, I defined the trip value as:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;trip value = (total fare) / (time waited + trip duration)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;and plotted it against hour of the day and day of the month (weekends and holidays and holidays in red).&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img src="../../images/taxi_2013/valueperhour_jan2013.png" alt="value per hour" style="width: 800px;"/&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img src="../../images/taxi_2013/valueperday_jan2013.png" alt="value per day" style="width: 800px;"/&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;The few cabs out at 5am are doing well (maybe because no one else is around) and then there is a peak after work. From the day of the month barplot we can easily infer that drunk folks like to take cabs after new year's eve celebrations. Also, the drivers seem to make increasingly more $/h as the week progresses.&lt;/p&gt;
&lt;p&gt;Finally, here is a panel with maps (you might want to enlarge it, but a blownup example is below). For each hour of the day the data is pixeled in location and the maps are colored according to the average trip value in each pixel. Pixels with too few cab pickups are discarded: there might be some very valuable trip, but still it is not advisable to drive a particular area if there are just a couple of pickups a month.&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img src="../../images/taxi_2013/panel_jan2013.png" alt="value panel" style="width: 800px;"/&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;As seen above, the middle of the day is not great, but downtown is consistently the best spot. One can also locate JFK and La Guardia airports: they are not necessarily the best spot in town, but they are a sure catch (as expected): the waiting involved probably takes down the trip value even if the total fare is high. Here is a blownup map for 9pm.&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img src="../../images/taxi_2013/example_jan2013_21.png" alt="example map" style="width: 800px;"/&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;Lastly, I checked what parameters matter the most in determining the value of each trip (duration and total fare apart, of course). I trained a random forest and here is the result.&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img src="../../images/taxi_2013/features_jan2013.png" alt="features" style="width: 800px;"/&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;It turns out that cabs make more or less the same money whether it is holiday or not and what matters most is the hour of the day they are out working.&lt;/p&gt;
&lt;p&gt;Location is missing but I am planning to classify the rides by neighborhood. The panel above clearly shows that different neighborhoods will yield different trip values. The most straighforward endgame for this study would be an app that advises cab drivers on where to drive in order to maximize the dollars per hour, given features and location.&lt;/p&gt;
&lt;p&gt;Nevertheless, the obvious next step is to include more than one month worth of data. One month is pretty much all my 8Gb RAM macbook pro can take, but thankfully cloud computing is cheap and easily accessible.&lt;/p&gt;
&lt;p&gt;People have been using these data to implement &lt;a href="http://chriswhong.com/open-data/foil_nyc_taxi/"&gt;really cool visualizations&lt;/a&gt; and answer pressing questions like &lt;a href="http://toddwschneider.com/posts/analyzing-1-1-billion-nyc-taxi-and-uber-trips-with-a-vengeance/"&gt;is Die Hard a realistic movie?&lt;/a&gt;. Yet, I believe there is still a lot of useful projects that could be developed thanks to these datasets. Maximizing cab drivers income is a great possibility, but the real challenge would be to understand what changes our city could implement to make traffic more efficient and all our lives better.&lt;/p&gt;</summary><category term="data"></category><category term="social"></category></entry></feed>